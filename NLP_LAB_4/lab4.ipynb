{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report, f1_score as seqeval_f1\n",
    "from simpletransformers.ner import NERArgs, NERModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_local_dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7532 sentences from data/Detailed-NER-Dataset-RU/dataset/detailed-ner_dataset-ru.pickle\n",
      "Columns: ['tokens', 'ner_tags']\n",
      "(19): ['B-CITY', 'B-COUNTRY', 'B-DISTRICT', 'B-FIRST_NAME', 'B-HOUSE', 'B-LAST_NAME', 'B-MIDDLE_NAME', 'B-REGION', 'B-STREET', 'I-CITY', 'I-COUNTRY', 'I-DISTRICT', 'I-FIRST_NAME', 'I-HOUSE', 'I-LAST_NAME', 'I-MIDDLE_NAME', 'I-REGION', 'I-STREET', 'O']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[dnsmasq, , 3753720, , , , , , , 1, , 0, Mar10...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2022-09-09, 12:37:10]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Повар, судовой]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[профилирование:, SafeData]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Кораблестроение,, океанотехника, и, системоте...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Художник, росписи, по, эмали]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[https://www.youtube.com/watch?v=dQw4w9WgXcQ]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Apr, 01,, 2016]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[дининфра:, ДинИнфра]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[CN=Самооценка, 2020,OU=DL,OU=SGROUPS,DC=int,D...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-CITY, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Образование]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Filesystem, , , , , , , , , , , , , , , , , S...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[Технология, и, техника, разведки, месторожден...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[hp, sm:, Service, Manager]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[Организация, и, управление, наукоемкими, прои...</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[льготные, условия, кредитования;]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[learn_d+, 2067193, 2779303, , 0, Mar27, ?, , ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Авиационная, и, ракетно-космическая, теплотех...</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[сонар:, SonarQube]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[orion]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[1.3289642200983877e+17]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[АО, «Сбербанк, России»\\n, ул., Ленина, ,, д.,...</td>\n",
       "      <td>[O, O, B-COUNTRY, O, B-STREET, O, B-HOUSE, I-H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[м17:, M17]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[Дерматовенерология]</td>\n",
       "      <td>[O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[GROUP, BY, ('столбец,, по, которому, хотим, с...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[any, connect:, VPN]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[Сельское,, лесное, и, рыбное, хозяйство]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[Норматив, потребления]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[Расчет, ЕНВД., Продукт, \"Понимаю, упрощенку\"]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[jabber:, Jabber]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tokens  \\\n",
       "0   [dnsmasq, , 3753720, , , , , , , 1, , 0, Mar10...   \n",
       "1                              [2022-09-09, 12:37:10]   \n",
       "2                                    [Повар, судовой]   \n",
       "3                         [профилирование:, SafeData]   \n",
       "4   [Кораблестроение,, океанотехника, и, системоте...   \n",
       "5                      [Художник, росписи, по, эмали]   \n",
       "6       [https://www.youtube.com/watch?v=dQw4w9WgXcQ]   \n",
       "7                                    [Apr, 01,, 2016]   \n",
       "8                               [дининфра:, ДинИнфра]   \n",
       "9   [CN=Самооценка, 2020,OU=DL,OU=SGROUPS,DC=int,D...   \n",
       "10                                      [Образование]   \n",
       "11  [Filesystem, , , , , , , , , , , , , , , , , S...   \n",
       "12  [Технология, и, техника, разведки, месторожден...   \n",
       "13                        [hp, sm:, Service, Manager]   \n",
       "14  [Организация, и, управление, наукоемкими, прои...   \n",
       "15                 [льготные, условия, кредитования;]   \n",
       "16  [learn_d+, 2067193, 2779303, , 0, Mar27, ?, , ...   \n",
       "17  [Авиационная, и, ракетно-космическая, теплотех...   \n",
       "18                                [сонар:, SonarQube]   \n",
       "19                                            [orion]   \n",
       "20                           [1.3289642200983877e+17]   \n",
       "21  [АО, «Сбербанк, России»\\n, ул., Ленина, ,, д.,...   \n",
       "22                                        [м17:, M17]   \n",
       "23                               [Дерматовенерология]   \n",
       "24  [GROUP, BY, ('столбец,, по, которому, хотим, с...   \n",
       "25                               [any, connect:, VPN]   \n",
       "26          [Сельское,, лесное, и, рыбное, хозяйство]   \n",
       "27                            [Норматив, потребления]   \n",
       "28     [Расчет, ЕНВД., Продукт, \"Понимаю, упрощенку\"]   \n",
       "29                                  [jabber:, Jabber]   \n",
       "\n",
       "                                             ner_tags  \n",
       "0   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1                                              [O, O]  \n",
       "2                                              [O, O]  \n",
       "3                                              [O, O]  \n",
       "4                               [O, O, O, O, O, O, O]  \n",
       "5                                        [O, O, O, O]  \n",
       "6                                                 [O]  \n",
       "7                                           [O, O, O]  \n",
       "8                                              [O, O]  \n",
       "9   [O, O, O, O, O, O, O, O, O, B-CITY, O, O, O, O...  \n",
       "10                                                [O]  \n",
       "11  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "12                              [O, O, O, O, O, O, O]  \n",
       "13                                       [O, O, O, O]  \n",
       "14                                    [O, O, O, O, O]  \n",
       "15                                          [O, O, O]  \n",
       "16  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "17                                       [O, O, O, O]  \n",
       "18                                             [O, O]  \n",
       "19                                                [O]  \n",
       "20                                                [O]  \n",
       "21  [O, O, B-COUNTRY, O, B-STREET, O, B-HOUSE, I-H...  \n",
       "22                                             [O, O]  \n",
       "23                                                [O]  \n",
       "24                        [O, O, O, O, O, O, O, O, O]  \n",
       "25                                          [O, O, O]  \n",
       "26                                    [O, O, O, O, O]  \n",
       "27                                             [O, O]  \n",
       "28                                    [O, O, O, O, O]  \n",
       "29                                             [O, O]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"data/Detailed-NER-Dataset-RU\")\n",
    "PICKLE_PATH = DATA_ROOT / \"dataset\" / \"detailed-ner_dataset-ru.pickle\"\n",
    "UTILS_PATH = (DATA_ROOT / \"utils\").resolve()\n",
    "\n",
    "if str(UTILS_PATH) not in sys.path:\n",
    "    sys.path.append(str(UTILS_PATH))\n",
    "\n",
    "from relabeling import biolu2bio\n",
    "\n",
    "raw_df = pd.read_pickle(PICKLE_PATH)\n",
    "print(f\"Loaded {len(raw_df)} sentences from {PICKLE_PATH}\")\n",
    "print(f\"Columns: {raw_df.columns.tolist()}\")\n",
    "\n",
    "raw_df[\"ner_tags\"] = raw_df[\"ner_tags\"].apply(biolu2bio)\n",
    "unique_tags = sorted({tag for sequence in raw_df[\"ner_tags\"] for tag in sequence})\n",
    "print(f\"({len(unique_tags)}): {unique_tags}\")\n",
    "\n",
    "raw_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simpletransformers_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    7532.000000\n",
      "mean        7.024827\n",
      "std        11.170213\n",
      "min         1.000000\n",
      "25%         2.000000\n",
      "50%         3.000000\n",
      "75%         8.000000\n",
      "max       424.000000\n",
      "Name: tokens, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dnsmasq</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3753720</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id    words labels\n",
       "0            0  dnsmasq      O\n",
       "1            0               O\n",
       "2            0  3753720      O\n",
       "3            0               O\n",
       "4            0               O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleTransformers-ready rows: 52911\n"
     ]
    }
   ],
   "source": [
    "required_cols = {\"tokens\", \"ner_tags\"}\n",
    "missing_cols = required_cols - set(raw_df.columns)\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"{missing_cols}\")\n",
    "\n",
    "lengths_ok = raw_df.apply(lambda row: len(row[\"tokens\"]) == len(row[\"ner_tags\"]), axis=1)\n",
    "if not lengths_ok.all():\n",
    "    bad_idx = lengths_ok.index[~lengths_ok].tolist()\n",
    "    raise ValueError(f\"mismatch at rows: {bad_idx[:5]}\")\n",
    "\n",
    "print(raw_df[\"tokens\"].map(len).describe())\n",
    "\n",
    "def to_simple_format(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for sent_id, (tokens, labels) in enumerate(zip(df[\"tokens\"], df[\"ner_tags\"])):\n",
    "        for token, label in zip(tokens, labels):\n",
    "            records.append({\"sentence_id\": sent_id, \"words\": token, \"labels\": label})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "simple_df = to_simple_format(raw_df)\n",
    "display(simple_df.head())\n",
    "print(f\"SimpleTransformers-ready rows: {len(simple_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sample_entity_sentences",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    for punct in [\" ,\", \" .\", \" :\", \" ;\", \" !\", \" ?\"]:\n",
    "        text = text.replace(punct, punct.strip())\n",
    "    return text\n",
    "\n",
    "entity_rows = raw_df[raw_df[\"ner_tags\"].map(lambda tags: any(t != \"O\" for t in tags))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels (19): ['B-CITY', 'B-COUNTRY', 'B-DISTRICT', 'B-FIRST_NAME', 'B-HOUSE', 'B-LAST_NAME', 'B-MIDDLE_NAME', 'B-REGION', 'B-STREET', 'I-CITY', 'I-COUNTRY', 'I-DISTRICT', 'I-FIRST_NAME', 'I-HOUSE', 'I-LAST_NAME', 'I-MIDDLE_NAME', 'I-REGION', 'I-STREET', 'O']\n",
      "Using device: cuda\n",
      "model gotted\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = \"bert\"\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "LABELS = sorted(simple_df[\"labels\"].unique())\n",
    "print(f\"Labels ({len(LABELS)}): {LABELS}\")\n",
    "\n",
    "SAVE_DIR = Path(\"models/rubert-detailed-ner\")\n",
    "WEIGHT_FILES = [\"pytorch_model.bin\", \"model.safetensors\"]\n",
    "\n",
    "def saved_model_exists(path: Path, filenames=WEIGHT_FILES) -> bool:\n",
    "    return any((path / name).exists() for name in filenames)\n",
    "\n",
    "model_args = NERArgs()\n",
    "model_args.labels_list = LABELS\n",
    "model_args.num_train_epochs = 8\n",
    "model_args.learning_rate = 3e-5\n",
    "model_args.train_batch_size = 16\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.max_seq_length = 256\n",
    "model_args.save_steps = 2000\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.output_dir = \"outputs/rubert-base-cased\"\n",
    "model_args.best_model_dir = \"outputs/rubert-base-cased/best\"\n",
    "model_args.use_multiprocessing = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if saved_model_exists(SAVE_DIR):\n",
    "    model_source = str(SAVE_DIR)\n",
    "    MODEL_EXISTS = True\n",
    "    print(f\"model gotted\")\n",
    "else:\n",
    "    model_source = MODEL_NAME\n",
    "    MODEL_EXISTS = False\n",
    "    print(\"training\")\n",
    "model = NERModel(\n",
    "    MODEL_TYPE,\n",
    "    model_source,\n",
    "    args=model_args,\n",
    "    labels=LABELS,\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    ")\n",
    "MODEL_WAS_TRAINED = False\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 6402, Val sentences: 1130\n",
      "Train rows: 45360, Val rows: 7551\n",
      "models/rubert-detailed-ner. Skipping training.\n"
     ]
    }
   ],
   "source": [
    "train_sentences, val_sentences = train_test_split(raw_df, test_size=0.15, shuffle=True, random_state=42)\n",
    "print(f\"Train sentences: {len(train_sentences)}, Val sentences: {len(val_sentences)}\")\n",
    "\n",
    "train_df = to_simple_format(train_sentences.reset_index(drop=True))\n",
    "val_df = to_simple_format(val_sentences.reset_index(drop=True))\n",
    "print(f\"Train rows: {len(train_df)}, Val rows: {len(val_df)}\")\n",
    "\n",
    "def seqeval_metrics(true_labels, predictions):\n",
    "    return {\"f1\": seqeval_f1(true_labels, predictions)}\n",
    "\n",
    "if MODEL_EXISTS:\n",
    "    print(f\"{SAVE_DIR}. Skipping training.\")\n",
    "else:\n",
    "    model.train_model(train_df, eval_data=val_df, f1=seqeval_metrics)\n",
    "    MODEL_WAS_TRAINED = True\n",
    "    MODEL_EXISTS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "evaluate_model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c2fdf291dd401ab2ce298536b14778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75272ee0314d4a818a0cab4b26a14be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il_dimas/Programming_Projects/NLP_LABS/.venv/lib/python3.13/site-packages/simpletransformers/ner/ner_model.py:1303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.07366522965498411, 'precision': np.float64(0.8768768768768769), 'recall': np.float64(0.8835098335854765), 'f1_score': np.float64(0.880180859080633)}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CITY       0.97      0.95      0.96       110\n",
      "     COUNTRY       0.93      0.98      0.95        93\n",
      "    DISTRICT       0.92      0.86      0.89        14\n",
      "  FIRST_NAME       0.81      0.83      0.82       139\n",
      "       HOUSE       0.73      0.80      0.76        20\n",
      "   LAST_NAME       0.84      0.84      0.84       174\n",
      " MIDDLE_NAME       0.97      0.89      0.93        36\n",
      "      REGION       0.86      0.91      0.88        54\n",
      "      STREET       0.90      0.90      0.90        21\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       661\n",
      "   macro avg       0.88      0.88      0.88       661\n",
      "weighted avg       0.88      0.88      0.88       661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result, model_outputs, predictions = model.eval_model(val_df)\n",
    "print(\"Validation metrics:\", eval_result)\n",
    "\n",
    "def extract_label(token):\n",
    "    if isinstance(token, dict):\n",
    "        for key in (\"label\", \"labels\", \"tag\", \"prediction\", \"entity\"):\n",
    "            if key in token:\n",
    "                return token[key]\n",
    "        return token.get(\"value\", \"O\")\n",
    "    if isinstance(token, (list, tuple)) and len(token) > 1:\n",
    "        return token[1]\n",
    "    return token\n",
    "\n",
    "pred_label_sequences = [[extract_label(token) for token in sent] for sent in predictions]\n",
    "true_label_sequences = [group[\"labels\"].tolist() for _, group in val_df.groupby(\"sentence_id\", sort=True)]\n",
    "print(classification_report(true_label_sequences, pred_label_sequences, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "save_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no train\n",
      "saved to models/rubert-detailed-ner (copied: ['model.safetensors', 'config.json', 'tokenizer_config.json', 'special_tokens_map.json', 'vocab.txt'])\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = Path(\"models/rubert-detailed-ner\")\n",
    "if not MODEL_WAS_TRAINED:\n",
    "    print(f\"no train\")\n",
    "    SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model.save_model(str(SAVE_DIR))\n",
    "    model.tokenizer.save_pretrained(str(SAVE_DIR))\n",
    "\n",
    "    source_dir = Path(model.args.output_dir)\n",
    "    artifacts = [\n",
    "        \"pytorch_model.bin\",\n",
    "        \"model.safetensors\",\n",
    "        \"config.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"vocab.txt\",\n",
    "    ]\n",
    "    copied = []\n",
    "    for name in artifacts:\n",
    "        src = source_dir / name\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, SAVE_DIR / name)\n",
    "            copied.append(name)\n",
    "\n",
    "    if not copied:\n",
    "        raise FileNotFoundError(f\" {source_dir}.\")\n",
    "\n",
    "    model.args.save(str(SAVE_DIR / \"model_args.json\"))\n",
    "    LABELS_PATH = SAVE_DIR / \"labels.json\"\n",
    "    LABELS_PATH.write_text(json.dumps(LABELS, ensure_ascii=False, indent=2))\n",
    "    print(f\"saved to {SAVE_DIR} (copied: {copied})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reload_and_infer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded from disk.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fb3ee4131f48d68b88c8646118a259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834450b0575e464fb315301e902a5953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Алексей', 'Петров', 'прилетел', 'в', 'Казань', 'по', 'рабочим', 'делам']\n",
      "['B-FIRST_NAME', 'B-LAST_NAME', 'O', 'O', 'B-CITY', 'O', 'O', 'O']\n",
      "\n",
      "['Компания', 'VK', 'запустила', 'новый', 'сервис', 'в', 'Новосибирске']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-CITY']\n",
      "\n",
      "['Премьер-министр', 'провёл', 'встречу', 'в', 'Доме', 'правительства']\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['Тинькофф', 'объявил', 'о', 'партнёрстве', 'с', 'X5', 'Group']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "['Екатерина', 'созвонилась', 'с', 'Андреем', 'и', 'договорилась', 'о', 'встрече', 'в', 'Екатеринбурге']\n",
      "['B-FIRST_NAME', 'O', 'O', 'B-FIRST_NAME', 'O', 'O', 'O', 'O', 'O', 'B-CITY']\n",
      "\n",
      "                                            sentence     token    pred_label\n",
      "0  Алексей Петров прилетел в Казань по рабочим делам   Алексей  B-FIRST_NAME\n",
      "1  Алексей Петров прилетел в Казань по рабочим делам    Петров   B-LAST_NAME\n",
      "2  Алексей Петров прилетел в Казань по рабочим делам  прилетел             O\n",
      "3  Алексей Петров прилетел в Казань по рабочим делам         в             O\n",
      "4  Алексей Петров прилетел в Казань по рабочим делам    Казань        B-CITY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/il_dimas/Programming_Projects/NLP_LABS/.venv/lib/python3.13/site-packages/simpletransformers/ner/ner_model.py:1644: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "LOAD_DIR = Path(\"models/rubert-detailed-ner\")\n",
    "weight_exists = any((LOAD_DIR / name).exists() for name in [\"pytorch_model.bin\", \"model.safetensors\"])\n",
    "if not weight_exists:\n",
    "    raise FileNotFoundError(f\"{LOAD_DIR}\")\n",
    "\n",
    "loaded_model = NERModel(\n",
    "    MODEL_TYPE,\n",
    "    str(LOAD_DIR),\n",
    "    args=model_args,\n",
    "    labels=LABELS,\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    ")\n",
    "print(\"Model reloaded from disk.\")\n",
    "\n",
    "def normalize_prediction(entry):\n",
    "    if isinstance(entry, dict):\n",
    "        token = entry.get(\"word\") or entry.get(\"token\") or entry.get(\"text\") or entry.get(\"sentence\")\n",
    "        for key in (\"label\", \"tag\", \"prediction\", \"entity\", \"labels\"):\n",
    "            if key in entry:\n",
    "                return token, entry[key]\n",
    "        if token is None and len(entry) == 1:\n",
    "            tok, label = next(iter(entry.items()))\n",
    "            return tok, label\n",
    "        return token, \"O\"\n",
    "    if isinstance(entry, (list, tuple)) and len(entry) > 1:\n",
    "        return entry[0], entry[1]\n",
    "    return entry, \"O\"\n",
    "\n",
    "default_sentences = [\n",
    "\"Алексей Петров прилетел в Казань по рабочим делам\",\n",
    "\"Компания VK запустила новый сервис в Новосибирске\",\n",
    "\"Премьер-министр провёл встречу в Доме правительства\",\n",
    "\"Тинькофф объявил о партнёрстве с X5 Group\",\n",
    "\"Екатерина созвонилась с Андреем и договорилась о встрече в Екатеринбурге\",\n",
    "]\n",
    "custom_sentences = default_sentences\n",
    "predictions, raw_outputs = loaded_model.predict(custom_sentences)\n",
    "normalized_predictions = [[normalize_prediction(p) for p in sent] for sent in predictions]\n",
    "\n",
    "import pandas as pd\n",
    "pred_rows = []\n",
    "for sentence, preds in zip(custom_sentences, normalized_predictions):\n",
    "    tokens = [tok for tok, _ in preds]\n",
    "    labels = [label for _, label in preds]\n",
    "    print(tokens)\n",
    "    print(labels)\n",
    "    print()\n",
    "    for tok, label in preds:\n",
    "        pred_rows.append({\"sentence\": sentence, \"token\": tok, \"pred_label\": label})\n",
    "pred_df = pd.DataFrame(pred_rows)\n",
    "print(pred_df.head())"
   ]
  }
 ],
 "metadata": {
  "imports_needed": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
