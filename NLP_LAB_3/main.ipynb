{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058d7d90",
   "metadata": {},
   "source": [
    "# Лабораторная 3: ELMo + BiLSTM NER (standalone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cec912",
   "metadata": {},
   "source": [
    "## 0. Предварительные требования\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776957e2",
   "metadata": {},
   "source": [
    "## 1. Базовые пути и окружение\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f15b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рабочая папка: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/NLP_LAB_3\n",
      "Данные будут в: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/data\n",
      "Модели будут в: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/models\n",
      "Результаты будем сохранять в: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/NLP_LAB_3/reports\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "LAB_DIR = Path.cwd()\n",
    "if not (LAB_DIR / 'train_elmo_lstm.py').exists():\n",
    "    candidate = LAB_DIR / 'NLP_LAB_3'\n",
    "    if (candidate / 'train_elmo_lstm.py').exists():\n",
    "        LAB_DIR = candidate.resolve()\n",
    "        sys.path.append(str(LAB_DIR))\n",
    "    else:\n",
    "        raise RuntimeError('Не найден train_elmo_lstm.py. Запустите блокнот из директории NLP_LAB_3 или корня проекта.')\n",
    "\n",
    "DATA_DIR = (LAB_DIR / '..' / 'data').resolve()\n",
    "MODELS_DIR = (LAB_DIR / '..' / 'models').resolve()\n",
    "REPORTS_DIR = LAB_DIR / 'reports'\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Рабочая папка: {LAB_DIR}\")\n",
    "print(f\"Данные будут в: {DATA_DIR}\")\n",
    "print(f\"Модели будут в: {MODELS_DIR}\")\n",
    "print(f\"Результаты будем сохранять в: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abab6e",
   "metadata": {},
   "source": [
    "## 2. Установка зависимостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63805f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d0be9",
   "metadata": {},
   "source": [
    "## 3. Загрузка датасета и предобученной ELMo-модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c8e5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA AND MODEL LOADED!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "DATA_REPO = DATA_DIR / 'Detailed-NER-Dataset-RU'\n",
    "if not DATA_REPO.exists():\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/AlexKly/Detailed-NER-Dataset-RU.git', str(DATA_REPO)], check=True)\n",
    "\n",
    "MODEL_DIR = MODELS_DIR / 'ruwikiruscorpora_tokens_elmo_1024_2019'\n",
    "MODEL_ZIP = MODELS_DIR / 'ruwikiruscorpora_tokens_elmo_1024_2019.zip'\n",
    "MODEL_URL = 'https://vectors.nlpl.eu/repository/20/195.zip'\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(MODEL_URL, MODEL_ZIP)\n",
    "    with zipfile.ZipFile(MODEL_ZIP, 'r') as zf:\n",
    "        zf.extractall(MODELS_DIR / 'ruwikiruscorpora_tokens_elmo_1024_2019')\n",
    "    MODEL_ZIP.unlink(missing_ok=True)\n",
    "\n",
    "print('DATA AND MODEL LOADED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e664176",
   "metadata": {},
   "source": [
    "## 4. Импорт библиотек и конфигурация устройства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa9341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/il_dimas/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/il_dimas/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.6) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13994501",
   "metadata": {},
   "source": [
    "## 5 перевод тегов BIOLU → BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2acff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пример конверсии: ['U-CITY', 'B-LAST_NAME', 'L-LAST_NAME', 'O'] -> ['B-CITY', 'B-LAST_NAME', 'I-LAST_NAME', 'O']\n"
     ]
    }
   ],
   "source": [
    "def biolu2bio(tags: Sequence[str]) -> List[str]:\n",
    "    converted = []\n",
    "    for tag in tags:\n",
    "        prefix = tag.split('-')[0]\n",
    "        label = tag.split('-')[-1]\n",
    "        if prefix == 'U':\n",
    "            converted.append(f'B-{label}')\n",
    "        elif prefix == 'L':\n",
    "            converted.append(f'I-{label}')\n",
    "        else:\n",
    "            converted.append(tag)\n",
    "    return converted\n",
    "\n",
    "example = ['U-CITY', 'B-LAST_NAME', 'L-LAST_NAME', 'O']\n",
    "print(example, '->', biolu2bio(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbd061",
   "metadata": {},
   "source": [
    "## 6. Загрузка датасета и проверка BIO-тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7928418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все теги соответствуют BIO формату.\n",
      "Количество предложений: 7532\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "pickle_path = DATA_REPO / 'dataset' / 'detailed-ner_dataset-ru.pickle'\n",
    "if not pickle_path.exists():\n",
    "    raise FileNotFoundError('Не найден detailed-ner_dataset-ru.pickle.')\n",
    "\n",
    "df = pd.read_pickle(pickle_path)\n",
    "tokens = df['tokens'].tolist()\n",
    "raw_tags = df['ner_tags'].tolist()\n",
    "\n",
    "bio_tags = [biolu2bio(seq) for seq in raw_tags]\n",
    "\n",
    "invalid = []\n",
    "for seq in bio_tags:\n",
    "    for tag in seq:\n",
    "        if tag != 'O' and not tag.startswith(('B-', 'I-')):\n",
    "            invalid.append(tag)\n",
    "\n",
    "if invalid:\n",
    "    print(Counter(invalid))\n",
    "else:\n",
    "    print('BIO - good')\n",
    "\n",
    "print('предложения', len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0efbd",
   "metadata": {},
   "source": [
    "## 7. Статистика датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "642de885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B-LAST_NAME', 1084), ('B-FIRST_NAME', 918), ('B-COUNTRY', 804), ('B-CITY', 677), ('B-REGION', 381), ('B-MIDDLE_NAME', 311), ('I-HOUSE', 218), ('B-STREET', 135), ('B-HOUSE', 120), ('B-DISTRICT', 110)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tag_counter = Counter(tag for seq in bio_tags for tag in seq if tag != 'O')\n",
    "print(tag_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4be5d4",
   "metadata": {},
   "source": [
    "## 8. Определение словаря тегов и разбиение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d268ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5422, Val: 603, Test: 1507\n",
      "Количество тегов: 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = np.arange(len(tokens))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "def select(items, idxs):\n",
    "    return [items[i] for i in idxs]\n",
    "\n",
    "train_tokens, val_tokens, test_tokens = select(tokens, train_idx), select(tokens, val_idx), select(tokens, test_idx)\n",
    "train_tags, val_tags, test_tags = select(bio_tags, train_idx), select(bio_tags, val_idx), select(bio_tags, test_idx)\n",
    "\n",
    "print(f'Train: {len(train_tokens)}, Val: {len(val_tokens)}, Test: {len(test_tokens)}')\n",
    "\n",
    "unique_tags = sorted({tag for seq in bio_tags for tag in seq})\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "print('Количество тегов:', len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce0597",
   "metadata": {},
   "source": [
    "## 9. Обёртки для ELMo и Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1ebf442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoSentenceEmbedder:\n",
    "    def __init__(self, options_path: Path, weight_path: Path, device: torch.device):\n",
    "        self.device = device\n",
    "        self.elmo = Elmo(\n",
    "            options_file=str(options_path),\n",
    "            weight_file=str(weight_path),\n",
    "            num_output_representations=1,\n",
    "            dropout=0.0,\n",
    "        ).to(self.device)\n",
    "        self.elmo.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(self, tokens: Sequence[str]) -> torch.Tensor:\n",
    "        character_ids = batch_to_ids([list(tokens)]).to(self.device)\n",
    "        outputs = self.elmo(character_ids)\n",
    "        embeddings = outputs['elmo_representations'][0][0]\n",
    "        return embeddings.cpu()\n",
    "\n",
    "\n",
    "class ElmoSequenceDataset(Dataset):\n",
    "    def __init__(self, tokens: Sequence[Sequence[str]], tags: Sequence[Sequence[str]], embedder: ElmoSentenceEmbedder, tag2idx: Dict[str, int]):\n",
    "        self.tokens = tokens\n",
    "        self.tags = tags\n",
    "        self.embedder = embedder\n",
    "        self.tag2idx = tag2idx\n",
    "        self._cache: Dict[int, torch.Tensor] = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def _embed(self, idx: int) -> torch.Tensor:\n",
    "        if idx not in self._cache:\n",
    "            embedding = self.embedder.embed(self.tokens[idx])\n",
    "            self._cache[idx] = embedding.clone().detach()\n",
    "        return self._cache[idx]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        embeddings = self._embed(idx)\n",
    "        tag_ids = torch.tensor([self.tag2idx[tag] for tag in self.tags[idx]], dtype=torch.long)\n",
    "        return embeddings, tag_ids, embeddings.size(0)\n",
    "\n",
    "\n",
    "def collate_batch(batch: Iterable[Tuple[torch.Tensor, torch.Tensor, int]]):\n",
    "    embeddings, tags, lengths = zip(*batch)\n",
    "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
    "    padded_embeddings = pad_sequence(embeddings, batch_first=True)\n",
    "    padded_tags = pad_sequence(tags, batch_first=True, padding_value=-100)\n",
    "    return padded_embeddings, padded_tags, lengths_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f56a6",
   "metadata": {},
   "source": [
    "## 10. BiLSTM модель и утилиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLstmTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_size: int, num_labels: int, num_layers: int = 1, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size * 2, num_labels)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        outputs, _ = self.lstm(packed)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "        outputs = self.dropout(outputs)\n",
    "        return self.classifier(outputs)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    loss: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    pbar = tqdm(loader, desc='Train', leave=False)\n",
    "    for embeddings, tags, lengths in pbar:\n",
    "        embeddings = embeddings.to(DEVICE)\n",
    "        tags = tags.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(embeddings, lengths)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), tags.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        valid_tokens = (tags != -100).sum().item()\n",
    "        total_loss += loss.item() * valid_tokens\n",
    "        total_tokens += valid_tokens\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.04f}'})\n",
    "    return total_loss / max(total_tokens, 1)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, idx2tag, stage='Eval'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    gold_sequences, pred_sequences = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for embeddings, tags, lengths in tqdm(loader, desc=stage, leave=False):\n",
    "            embeddings = embeddings.to(DEVICE)\n",
    "            tags = tags.to(DEVICE)\n",
    "            logits = model(embeddings, lengths)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tags.view(-1))\n",
    "\n",
    "            mask = tags != -100\n",
    "            token_count = mask.sum().item()\n",
    "            total_loss += loss.item() * token_count\n",
    "            total_tokens += token_count\n",
    "\n",
    "            preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "            gold = tags.cpu().numpy()\n",
    "            lengths_np = lengths.numpy()\n",
    "            for pred_seq, gold_seq, length in zip(preds, gold, lengths_np):\n",
    "                pred_labels = [idx2tag[idx] for idx in pred_seq[:length]]\n",
    "                gold_labels = [idx2tag[idx] for idx in gold_seq[:length]]\n",
    "                pred_sequences.append(pred_labels)\n",
    "                gold_sequences.append(gold_labels)\n",
    "\n",
    "    loss_value = total_loss / max(total_tokens, 1)\n",
    "    precision = precision_score(gold_sequences, pred_sequences, zero_division=0)\n",
    "    recall = recall_score(gold_sequences, pred_sequences, zero_division=0)\n",
    "    f1 = f1_score(gold_sequences, pred_sequences, zero_division=0)\n",
    "    return Metrics(loss_value, precision, recall, f1), gold_sequences, pred_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c7645",
   "metadata": {},
   "source": [
    "## 11. Гиперпараметры и DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d29c906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL AND DATA READY!\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'epochs': 5,\n",
    "    'batch_size': 12,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 1e-3,\n",
    "    'seed': 42,\n",
    "}\n",
    "set_seed(config['seed'])\n",
    "options_path = MODEL_DIR / 'options.json'\n",
    "weights_path = MODEL_DIR / 'model.hdf5'\n",
    "if not (options_path.exists() and weights_path.exists()):\n",
    "    raise FileNotFoundError('Не найдены options.json/model.hdf5 в директории модели.')\n",
    "\n",
    "elmo_embedder = ElmoSentenceEmbedder(options_path, weights_path, DEVICE)\n",
    "\n",
    "train_dataset = ElmoSequenceDataset(train_tokens, train_tags, elmo_embedder, tag2idx)\n",
    "val_dataset = ElmoSequenceDataset(val_tokens, val_tags, elmo_embedder, tag2idx)\n",
    "test_dataset = ElmoSequenceDataset(test_tokens, test_tags, elmo_embedder, tag2idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "model = BiLstmTagger(\n",
    "    embedding_dim=1024,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    num_labels=len(tag2idx),\n",
    "    num_layers=config['num_layers'],\n",
    "    dropout=config['dropout'],\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "print('MODEL AND DATA READY!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03784138",
   "metadata": {},
   "source": [
    "## 12. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2156\n",
      "Validation -> loss: 0.1636, precision: 0.7731, recall: 0.6411, f1: 0.7009\n",
      "✓ Лучшее качество обновлено.\n",
      "=== Epoch 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0852\n",
      "Validation -> loss: 0.1182, precision: 0.7705, recall: 0.7896, f1: 0.7800\n",
      "✓ Лучшее качество обновлено.\n",
      "=== Epoch 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0584\n",
      "Validation -> loss: 0.0974, precision: 0.8317, recall: 0.8193, f1: 0.8254\n",
      "✓ Лучшее качество обновлено.\n",
      "=== Epoch 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0430\n",
      "Validation -> loss: 0.0923, precision: 0.8038, recall: 0.8317, f1: 0.8175\n",
      "=== Epoch 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0280\n",
      "Validation -> loss: 0.0844, precision: 0.8601, recall: 0.8366, f1: 0.8482\n",
      "✓ Лучшее качество обновлено.\n",
      "Загружены веса лучшей эпохи (5).\n",
      "Обучение заняло 5.33 мин.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "best_state = None\n",
    "best_val_f1 = -1.0\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    print(f'=== Epoch {epoch}/{config[\"epochs\"]} ===')\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_metrics, _, _ = evaluate(model, val_loader, criterion, idx2tag, stage='Validation')\n",
    "\n",
    "    epoch_summary = {\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_metrics.loss,\n",
    "        'val_precision': val_metrics.precision,\n",
    "        'val_recall': val_metrics.recall,\n",
    "        'val_f1': val_metrics.f1,\n",
    "    }\n",
    "    history.append(epoch_summary)\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation -> loss: {val_metrics.loss:.4f}, precision: {val_metrics.precision:.4f}, recall: {val_metrics.recall:.4f}, f1: {val_metrics.f1:.4f}\")\n",
    "\n",
    "    if val_metrics.f1 > best_val_f1:\n",
    "        best_val_f1 = val_metrics.f1\n",
    "        best_state = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_metrics': val_metrics.to_dict(),\n",
    "        }\n",
    "\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state['model'])\n",
    "    print(f\"({best_state['epoch']}).\")\n",
    "\n",
    "print(f\"Обучение заняло {(time.time() - start_time)/60:.2f} мин.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9695bae4",
   "metadata": {},
   "source": [
    "## 13. Тестирование и отчёт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b98e2eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test -> loss: 0.0797, precision: 0.8571, recall: 0.8230, f1: 0.8397\n",
      "Seqeval report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CITY       0.95      0.90      0.92       148\n",
      "     COUNTRY       0.88      0.86      0.87       132\n",
      "    DISTRICT       0.92      0.71      0.80        17\n",
      "  FIRST_NAME       0.81      0.82      0.82       194\n",
      "       HOUSE       0.63      0.71      0.67        24\n",
      "   LAST_NAME       0.83      0.80      0.81       235\n",
      " MIDDLE_NAME       0.86      0.80      0.83        60\n",
      "      REGION       0.93      0.78      0.85        69\n",
      "      STREET       0.86      0.76      0.81        25\n",
      "\n",
      "   micro avg       0.86      0.82      0.84       904\n",
      "   macro avg       0.85      0.79      0.82       904\n",
      "weighted avg       0.86      0.82      0.84       904\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test_metrics, gold_sequences, pred_sequences = evaluate(model, test_loader, criterion, idx2tag, stage='Test')\n",
    "print(\n",
    "    f\"Test -> loss: {test_metrics.loss:.4f}, precision: {test_metrics.precision:.4f}, \"\n",
    "    f\"recall: {test_metrics.recall:.4f}, f1: {test_metrics.f1:.4f}\"\n",
    ")\n",
    "\n",
    "report_text = classification_report(gold_sequences, pred_sequences, zero_division=0)\n",
    "print('Seqeval report:')\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec6f0a",
   "metadata": {},
   "source": [
    "## 14. Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a25077d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2385"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_payload = {\n",
    "    'config': config,\n",
    "    'history': history,\n",
    "    'best_val': best_state['val_metrics'] if best_state else None,\n",
    "    'test': test_metrics.to_dict(),\n",
    "    'classification_report': report_text,\n",
    "}\n",
    "\n",
    "metrics_path = REPORTS_DIR / 'metrics.json'\n",
    "metrics_path.write_text(json.dumps(metrics_payload, indent=2, ensure_ascii=False), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383bbed",
   "metadata": {},
   "source": [
    "## 15. Инференс на новых текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0af5205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл по умолчанию: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/NLP_LAB_3/sample_text.txt\n",
      "Сохранено в /Users/il_dimas/Documents/Programming_projects/NLP_LABS/NLP_LAB_3/reports/external_predictions_sample_text.json\n",
      "---\n",
      "В Саратове продолжают разрушаться казармы Деконского, построенные более 150 лет назад и являющиеся объектом культурного наследия регионального значения.\n",
      "Сущности: [('В', 'CITY'), ('Саратове', 'MIDDLE_NAME'), ('продолжают', 'REGION'), ('разрушаться', 'MIDDLE_NAME'), ('казармы', 'STREET'), ('Деконского', 'STREET'), (',', 'STREET'), ('построенные', 'MIDDLE_NAME'), ('более', 'COUNTRY'), ('150', 'COUNTRY'), ('лет', 'REGION'), ('назад и', 'MIDDLE_NAME'), ('являющиеся', 'CITY'), ('объектом', 'CITY'), ('культурного', 'CITY'), ('наследия', 'STREET'), ('регионального', 'MIDDLE_NAME'), ('значения', 'MIDDLE_NAME'), ('.', 'FIRST_NAME')]\n",
      "---\n",
      "Сегодня в этом на месте убедился корреспондент ИА \"Взгляд-инфо\".\n",
      "Сущности: [('Сегодня', 'REGION'), ('в этом', 'REGION'), ('на', 'REGION'), ('месте', 'STREET'), ('убедился', 'REGION'), ('корреспондент', 'REGION'), ('ИА', 'FIRST_NAME'), ('\"', 'FIRST_NAME'), ('Взгляд-инфо', 'REGION'), ('\"', 'STREET'), ('.', 'FIRST_NAME')]\n",
      "---\n",
      "Ранее редакция направила официальные запросы в СК России о необходимости дать правовую оценку бездействию чиновников.\n",
      "Сущности: [('Ранее', 'STREET'), ('редакция', 'REGION'), ('направила', 'REGION'), ('официальные', 'FIRST_NAME'), ('запросы', 'MIDDLE_NAME'), ('в', 'HOUSE'), ('СК', 'STREET'), ('России', 'STREET'), ('о необходимости', 'HOUSE'), ('дать', 'HOUSE'), ('правовую', 'HOUSE'), ('оценку', 'DISTRICT'), ('бездействию', 'HOUSE'), ('чиновников', 'HOUSE'), ('.', 'HOUSE')]\n",
      "---\n",
      "Следком переадресовал обращения в прокуратуру Саратовской области и региональное правительство. На днях журналисты получили ответы профильного областного комитета и надзорного ведомства.\n",
      "Сущности: [('Следком', 'CITY'), ('переадресовал', 'REGION'), ('обращения', 'HOUSE'), ('в', 'REGION'), ('прокуратуру', 'STREET'), ('Саратовской', 'STREET'), ('области', 'HOUSE'), ('и', 'REGION'), ('региональное', 'CITY'), ('правительство', 'REGION'), ('.', 'REGION'), ('На', 'REGION'), ('днях', 'HOUSE'), ('журналисты', 'REGION'), ('получили', 'REGION'), ('ответы', 'STREET'), ('профильного', 'STREET'), ('областного', 'STREET'), ('комитета', 'STREET'), ('и', 'STREET'), ('надзорного', 'STREET'), ('ведомства', 'STREET'), ('.', 'HOUSE')]\n",
      "---\n",
      "Последнее крупное обрушение казарм произошло 15 июля – тогда рухнули стены и балки чердака.\n",
      "Сущности: [('Последнее крупное', 'MIDDLE_NAME'), ('обрушение', 'DISTRICT'), ('казарм произошло 15 июля –', 'STREET'), ('тогда', 'STREET'), ('рухнули', 'STREET'), ('стены', 'STREET'), ('и', 'COUNTRY'), ('балки', 'COUNTRY'), ('чердака .', 'STREET')]\n",
      "---\n",
      "Балансодержателем и правообладателем здания является Саратовский областной музей краеведения (директор – Дмитрий Кубанкин; учредитель – министерство культуры Наталии Щелкановой).\n",
      "Сущности: [('Балансодержателем и', 'HOUSE'), ('правообладателем', 'HOUSE'), ('здания', 'DISTRICT'), ('является Саратовский областной', 'HOUSE'), ('музей', 'CITY'), ('краеведения', 'MIDDLE_NAME'), ('(', 'STREET'), ('директор', 'STREET'), ('–', 'CITY'), ('Дмитрий', 'CITY'), ('Кубанкин', 'STREET'), (';', 'CITY'), ('учредитель', 'STREET'), ('–', 'CITY'), ('министерство', 'CITY'), ('культуры', 'CITY'), ('Наталии', 'CITY'), ('Щелкановой', 'STREET'), (')', 'FIRST_NAME'), ('.', 'HOUSE')]\n",
      "---\n",
      "Как сообщили \"Взгляду\" в комитете культурного наследия, 20 октября реставрационно-строительной компании \"Наследие\" выдано разрешение на обследование казарм Деконского с целью дальнейшей консервации, а еще в июле краеведческий музей получил предостережение.\n",
      "Сущности: [('Как', 'REGION'), ('сообщили', 'REGION'), ('\"', 'REGION'), ('Взгляду', 'REGION'), ('\"', 'REGION'), ('в', 'REGION'), ('комитете', 'STREET'), ('культурного', 'STREET'), ('наследия', 'REGION'), (',', 'REGION'), ('20', 'REGION'), ('октября', 'STREET'), ('реставрационно-строительной', 'STREET'), ('компании', 'REGION'), ('\"', 'REGION'), ('Наследие', 'REGION'), ('\"', 'REGION'), ('выдано', 'REGION'), ('разрешение', 'DISTRICT'), ('на', 'STREET'), ('обследование', 'STREET'), ('казарм', 'HOUSE'), ('Деконского', 'STREET'), ('с', 'MIDDLE_NAME'), ('целью', 'HOUSE'), ('дальнейшей', 'HOUSE'), ('консервации', 'REGION'), (',', 'REGION'), ('а', 'REGION'), ('еще в', 'REGION'), ('июле', 'STREET'), ('краеведческий', 'FIRST_NAME'), ('музей', 'MIDDLE_NAME'), ('получил', 'FIRST_NAME'), ('предостережение', 'MIDDLE_NAME'), ('.', 'HOUSE')]\n",
      "---\n",
      "А вот участие прокуратуры в спасении здания ограничилось представлением в адрес балансодержателя; инициировать уголовное дело о халатности подчиненные Сергея Филипенко не стали, переложив ответственность за это на коллег из СК.\n",
      "Сущности: [('А', 'REGION'), ('вот', 'MIDDLE_NAME'), ('участие', 'MIDDLE_NAME'), ('прокуратуры', 'MIDDLE_NAME'), ('в', 'MIDDLE_NAME'), ('спасении', 'MIDDLE_NAME'), ('здания', 'MIDDLE_NAME'), ('ограничилось', 'MIDDLE_NAME'), ('представлением', 'MIDDLE_NAME'), ('в', 'MIDDLE_NAME'), ('адрес', 'LAST_NAME'), ('балансодержателя', 'STREET'), (';', 'HOUSE'), ('инициировать', 'STREET'), ('уголовное', 'LAST_NAME'), ('дело', 'HOUSE'), ('о', 'MIDDLE_NAME'), ('халатности', 'STREET'), ('подчиненные', 'STREET'), ('Сергея', 'STREET'), ('Филипенко', 'STREET'), ('не', 'CITY'), ('стали', 'STREET'), (',', 'REGION'), ('переложив', 'STREET'), ('ответственность', 'HOUSE'), ('за', 'LAST_NAME'), ('это', 'REGION'), ('на', 'REGION'), ('коллег', 'REGION'), ('из', 'REGION'), ('СК', 'MIDDLE_NAME'), ('.', 'HOUSE')]\n",
      "---\n",
      "Тем временем сами казармы постепенно руинируются.\n",
      "Сущности: [('временем', 'HOUSE'), ('сами', 'CITY'), ('казармы', 'STREET'), ('постепенно', 'STREET'), ('руинируются', 'STREET'), ('.', 'HOUSE')]\n",
      "---\n",
      "\"Шедевр архитектуры находится в крайне плачевном состоянии. С разрушенного чердака свисают балки, разрушенная кладка придает зданию вид абсолютной бесхозности и ненужности. При этом достаточно опрятный ансамбль зданий на Ильинской площади и красивый цветник у музея \"Россия – моя история\" резко контрастируют с этими руинами. Раньше казармы были обтянуты сеткой с фальшфасадом со всех сторон. Теперь сетка осталась только с торца.\n",
      "Сущности: [('\"', 'REGION'), ('Шедевр', 'REGION'), ('архитектуры', 'REGION'), ('находится', 'REGION'), ('в', 'MIDDLE_NAME'), ('крайне', 'MIDDLE_NAME'), ('плачевном', 'MIDDLE_NAME'), ('состоянии', 'MIDDLE_NAME'), ('.', 'STREET'), ('С', 'HOUSE'), ('разрушенного', 'HOUSE'), ('чердака', 'FIRST_NAME'), ('свисают', 'MIDDLE_NAME'), ('балки', 'STREET'), (',', 'FIRST_NAME'), ('разрушенная', 'FIRST_NAME'), ('кладка', 'FIRST_NAME'), ('придает', 'FIRST_NAME'), ('зданию', 'FIRST_NAME'), ('вид', 'FIRST_NAME'), ('абсолютной', 'FIRST_NAME'), ('бесхозности', 'FIRST_NAME'), ('и', 'FIRST_NAME'), ('ненужности', 'FIRST_NAME'), ('.', 'HOUSE'), ('достаточно', 'STREET'), ('опрятный', 'STREET'), ('ансамбль', 'STREET'), ('зданий на', 'STREET'), ('Ильинской', 'STREET'), ('площади и', 'STREET'), ('красивый', 'REGION'), ('цветник', 'CITY'), ('у', 'COUNTRY'), ('\"', 'STREET'), ('Россия', 'STREET'), ('–', 'CITY'), ('моя', 'STREET'), ('история', 'STREET'), ('\"', 'STREET'), ('резко', 'MIDDLE_NAME'), ('контрастируют', 'MIDDLE_NAME'), ('с', 'STREET'), ('этими', 'HOUSE'), ('руинами', 'STREET'), ('.', 'STREET'), ('Раньше', 'HOUSE'), ('казармы', 'STREET'), ('были', 'STREET'), ('обтянуты', 'STREET'), ('сеткой', 'STREET'), ('с', 'STREET'), ('фальшфасадом', 'HOUSE'), ('со', 'HOUSE'), ('всех', 'HOUSE'), ('сторон', 'STREET'), ('.', 'HOUSE'), ('Теперь', 'HOUSE'), ('сетка', 'MIDDLE_NAME'), ('осталась', 'REGION'), ('только', 'STREET'), ('с', 'STREET'), ('торца', 'STREET'), ('.', 'HOUSE')]\n",
      "---\n",
      "Обвалившаяся стена снесла секцию забора, отгораживающего проезд между казармами и музеем. Значит, возможно, что следующее обрушение может случиться кому-нибудь на голову или на машину. Не дай бог, конечно\", - поделился впечатлениями от увиденного наш корреспондент.\n",
      "Сущности: [('Обвалившаяся', 'FIRST_NAME'), ('стена', 'FIRST_NAME'), ('снесла', 'STREET'), ('секцию', 'FIRST_NAME'), ('забора', 'STREET'), (',', 'FIRST_NAME'), ('отгораживающего', 'FIRST_NAME'), ('проезд', 'STREET'), ('между', 'FIRST_NAME'), ('казармами', 'STREET'), ('и', 'FIRST_NAME'), ('музеем', 'CITY'), ('.', 'REGION'), ('Значит', 'CITY'), (',', 'CITY'), ('возможно ,', 'STREET'), ('что следующее', 'CITY'), ('обрушение', 'DISTRICT'), ('может', 'CITY'), ('случиться', 'HOUSE'), ('кому-нибудь', 'HOUSE'), ('на', 'STREET'), ('голову', 'FIRST_NAME'), ('или на машину', 'STREET'), ('.', 'HOUSE'), ('Не', 'STREET'), ('бог', 'DISTRICT'), (',', 'FIRST_NAME'), ('конечно', 'FIRST_NAME'), ('\"', 'FIRST_NAME'), (',', 'FIRST_NAME'), ('-', 'FIRST_NAME'), ('поделился', 'FIRST_NAME'), ('впечатлениями', 'HOUSE'), ('от', 'FIRST_NAME'), ('увиденного', 'CITY'), ('наш', 'REGION'), ('корреспондент', 'REGION'), ('.', 'REGION')]\n",
      "---\n",
      "Мария Николаева, местная жительница, добавила в беседе с журналистом: \"Такое красивое и большое здание с богатой историей уходит, очень жалко. Я видела уже, по крайней мере, три пожара. Несколько раз приезжали чиновники в костюмах, что-то решали. Но после их отъезда все остается по-прежнему\".\n",
      "Сущности: [('Мария', 'STREET'), ('Николаева', 'STREET'), (',', 'REGION'), ('местная', 'REGION'), ('жительница', 'FIRST_NAME'), (',', 'MIDDLE_NAME'), ('добавила', 'REGION'), ('в', 'CITY'), ('беседе', 'HOUSE'), ('с', 'FIRST_NAME'), ('журналистом :', 'HOUSE'), ('\"', 'STREET'), ('Такое', 'CITY'), ('красивое', 'DISTRICT'), ('и', 'DISTRICT'), ('большое', 'MIDDLE_NAME'), ('здание', 'DISTRICT'), ('с', 'DISTRICT'), ('богатой', 'DISTRICT'), ('историей', 'MIDDLE_NAME'), ('уходит', 'MIDDLE_NAME'), (',', 'MIDDLE_NAME'), ('очень', 'REGION'), ('жалко', 'FIRST_NAME'), ('.', 'HOUSE'), ('Я', 'REGION'), ('видела', 'REGION'), ('уже', 'REGION'), (',', 'REGION'), ('по', 'MIDDLE_NAME'), ('крайней', 'HOUSE'), ('мере', 'STREET'), (',', 'COUNTRY'), ('три', 'HOUSE'), ('пожара', 'STREET'), ('.', 'REGION'), ('Несколько', 'REGION'), ('раз', 'REGION'), ('приезжали', 'DISTRICT'), ('чиновники', 'DISTRICT'), ('в', 'HOUSE'), ('костюмах', 'STREET'), (',', 'STREET'), ('что-то', 'COUNTRY'), ('решали', 'STREET'), ('.', 'HOUSE'), ('Но', 'HOUSE'), ('после', 'HOUSE'), ('их', 'HOUSE'), ('отъезда', 'REGION'), ('все', 'REGION'), ('остается', 'STREET'), ('по-прежнему', 'STREET'), ('\"', 'STREET'), ('.', 'HOUSE')]\n",
      "---\n",
      "Впервые упоминание об этом уникальном доме встречается в \"Адрес-календаре за 1867 год\". Его владельцем был капитан Генерального штаба в отставке Григорий Кузьмич Деконский (1822-1892). Он был крупным землевладельцем, имел несколько домов в городе, владел крупным участком земли в пригороде Саратова.\n",
      "Сущности: [('Впервые', 'MIDDLE_NAME'), ('упоминание', 'MIDDLE_NAME'), ('об', 'MIDDLE_NAME'), ('этом', 'MIDDLE_NAME'), ('уникальном', 'MIDDLE_NAME'), ('доме', 'MIDDLE_NAME'), ('встречается', 'MIDDLE_NAME'), ('в', 'MIDDLE_NAME'), ('\"', 'MIDDLE_NAME'), ('Адрес-календаре', 'MIDDLE_NAME'), ('за', 'REGION'), ('1867', 'FIRST_NAME'), ('год', 'REGION'), ('\"', 'REGION'), ('.', 'REGION'), ('Его', 'FIRST_NAME'), ('был', 'REGION'), ('капитан', 'MIDDLE_NAME'), ('Генерального', 'REGION'), ('штаба', 'REGION'), ('в', 'REGION'), ('отставке', 'STREET'), ('Григорий', 'MIDDLE_NAME'), ('Кузьмич', 'STREET'), ('Деконский', 'REGION'), ('(', 'REGION'), ('1822-1892', 'REGION'), (')', 'REGION'), ('.', 'REGION'), ('Он', 'REGION'), ('был', 'REGION'), ('крупным', 'STREET'), ('землевладельцем ,', 'STREET'), ('имел', 'REGION'), ('несколько', 'FIRST_NAME'), ('домов в', 'STREET'), ('городе', 'MIDDLE_NAME'), (',', 'FIRST_NAME'), ('владел', 'MIDDLE_NAME'), ('крупным', 'FIRST_NAME'), ('участком', 'FIRST_NAME'), ('земли', 'FIRST_NAME'), ('в', 'STREET'), ('пригороде', 'FIRST_NAME'), ('Саратова', 'MIDDLE_NAME'), ('.', 'FIRST_NAME')]\n",
      "---\n",
      "Одним из видов деятельности отставного офицера Генштаба было строительство объектов для нужд армии.\n",
      "Сущности: [('Одним из', 'DISTRICT'), ('видов', 'STREET'), ('деятельности', 'LAST_NAME'), ('отставного', 'STREET'), ('офицера', 'STREET'), ('Генштаба', 'HOUSE'), ('было', 'HOUSE'), ('строительство', 'HOUSE'), ('объектов', 'CITY'), ('для', 'CITY'), ('нужд', 'HOUSE'), ('армии', 'CITY'), ('.', 'HOUSE')]\n",
      "---\n",
      "Он на свои средства отстраивал казармы и другие объекты, а позже передавал их государству в длительную аренду. Так был воздвигнут большой казарменный комплекс на Ильинской площади. Многие годы здесь располагались артиллеристы…\n",
      "Сущности: [('на', 'LAST_NAME'), ('средства', 'REGION'), ('отстраивал', 'REGION'), ('казармы', 'MIDDLE_NAME'), ('и другие', 'STREET'), ('объекты', 'FIRST_NAME'), (', а', 'LAST_NAME'), ('позже', 'FIRST_NAME'), ('передавал', 'FIRST_NAME'), ('их', 'CITY'), ('государству', 'CITY'), ('в', 'CITY'), ('длительную', 'STREET'), ('аренду', 'HOUSE'), ('.', 'HOUSE'), ('Так', 'REGION'), ('был воздвигнут', 'MIDDLE_NAME'), ('большой', 'FIRST_NAME'), ('казарменный', 'REGION'), ('комплекс', 'FIRST_NAME'), ('на Ильинской', 'STREET'), ('площади', 'MIDDLE_NAME'), ('.', 'DISTRICT'), ('Многие', 'HOUSE'), ('годы здесь', 'STREET'), ('располагались', 'MIDDLE_NAME'), ('артиллеристы', 'STREET'), ('…', 'HOUSE')]\n",
      "---\n",
      "В 2018 году губернатор Валерий Радаев планировал открыть в казармах Деконского детский оздоровительный центр. Но не получилось.\n",
      "Сущности: [('В', 'CITY'), ('году', 'MIDDLE_NAME'), ('губернатор', 'FIRST_NAME'), ('Валерий', 'REGION'), ('Радаев', 'REGION'), ('планировал', 'REGION'), ('открыть в', 'FIRST_NAME'), ('казармах', 'COUNTRY'), ('Деконского', 'STREET'), ('детский', 'LAST_NAME'), ('оздоровительный', 'MIDDLE_NAME'), ('центр', 'MIDDLE_NAME'), ('.', 'HOUSE'), ('Но', 'HOUSE'), ('не получилось .', 'CITY')]\n",
      "---\n",
      "Чуть позже чиновники озвучили инициативу сдавать подобные объекты в аренду за рубль. Однако казармы продолжили разрушаться, а инвесторы не спешили вкладываться.\n",
      "Сущности: [('позже', 'STREET'), ('чиновники', 'STREET'), ('озвучили', 'FIRST_NAME'), ('инициативу', 'STREET'), ('сдавать', 'FIRST_NAME'), ('подобные', 'FIRST_NAME'), ('объекты', 'FIRST_NAME'), ('в', 'CITY'), ('аренду', 'CITY'), ('за', 'HOUSE'), ('рубль', 'FIRST_NAME'), ('. Однако казармы', 'HOUSE'), ('продолжили', 'STREET'), ('разрушаться', 'HOUSE'), (',', 'STREET'), ('а', 'HOUSE'), ('инвесторы', 'STREET'), ('не', 'STREET'), ('спешили', 'STREET'), ('вкладываться', 'HOUSE'), ('.', 'HOUSE')]\n",
      "---\n",
      "Возможно, что одна из главных причин – отсутствие в регионе гарантий права частной собственности и независимого правосудия.\n",
      "Сущности: [('что', 'HOUSE'), ('одна', 'REGION'), ('из', 'HOUSE'), ('главных', 'STREET'), ('причин –', 'HOUSE'), ('отсутствие', 'STREET'), ('в', 'HOUSE'), ('регионе', 'CITY'), ('гарантий', 'HOUSE'), ('права частной', 'STREET'), ('собственности', 'HOUSE'), ('и', 'DISTRICT'), ('независимого', 'STREET'), ('правосудия', 'STREET'), ('.', 'HOUSE')]\n",
      "---\n",
      "Есть примеры, когда под надуманными предлогами здания изымались у собственников и передавались муниципалитету, области или федеральным структурам.\n",
      "Сущности: [('Есть', 'REGION'), ('примеры', 'STREET'), (',', 'STREET'), ('когда', 'STREET'), ('под', 'HOUSE'), ('надуманными', 'CITY'), ('предлогами', 'STREET'), ('здания', 'STREET'), ('изымались у', 'HOUSE'), ('собственников', 'STREET'), ('и передавались муниципалитету ,', 'HOUSE'), ('области', 'CITY'), ('или', 'FIRST_NAME'), ('федеральным', 'CITY'), ('структурам', 'FIRST_NAME'), ('.', 'FIRST_NAME')]\n",
      "---\n",
      "Как правило, решения об изъятии объяснялись заботой о сохранении объектов культурного наследия, но на практике все происходит с точностью до наоборот.\n",
      "Сущности: [('Как', 'HOUSE'), ('правило', 'HOUSE'), (',', 'MIDDLE_NAME'), ('решения', 'STREET'), ('об', 'STREET'), ('изъятии', 'CITY'), ('объяснялись заботой о', 'HOUSE'), ('сохранении', 'CITY'), ('объектов', 'CITY'), ('культурного', 'STREET'), ('наследия', 'STREET'), (',', 'FIRST_NAME'), ('но', 'FIRST_NAME'), ('на', 'REGION'), ('практике', 'REGION'), ('все', 'CITY'), ('происходит', 'CITY'), ('с', 'FIRST_NAME'), ('точностью', 'FIRST_NAME'), ('до', 'CITY'), ('наоборот', 'FIRST_NAME'), ('.', 'HOUSE')]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    return [token.text for token in razdel_tokenize(text)]\n",
    "\n",
    "\n",
    "def extract_entities(tokens: Sequence[str], tags: Sequence[str]):\n",
    "    entities = []\n",
    "    buffer = []\n",
    "    current_label = None\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag == 'O':\n",
    "            if buffer:\n",
    "                entities.append((' '.join(buffer), current_label))\n",
    "                buffer = []\n",
    "                current_label = None\n",
    "            continue\n",
    "        prefix, label = tag.split('-', 1)\n",
    "        if prefix == 'B':\n",
    "            if buffer:\n",
    "                entities.append((' '.join(buffer), current_label))\n",
    "            buffer = [token]\n",
    "            current_label = label\n",
    "        elif prefix == 'I' and current_label == label:\n",
    "            buffer.append(token)\n",
    "        else:\n",
    "            if buffer:\n",
    "                entities.append((' '.join(buffer), current_label))\n",
    "            buffer = [token]\n",
    "            current_label = label\n",
    "    if buffer:\n",
    "        entities.append((' '.join(buffer), current_label))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def predict_tags_for_sentence(tokens: Sequence[str]):\n",
    "    embeddings = elmo_embedder.embed(tokens)\n",
    "    tensor = embeddings.unsqueeze(0).to(DEVICE)\n",
    "    lengths = torch.tensor([len(tokens)], dtype=torch.long)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor, lengths)\n",
    "        pred_ids = logits.argmax(dim=-1)[0][: len(tokens)].cpu().tolist()\n",
    "    return [idx2tag[idx] for idx in pred_ids]\n",
    "\n",
    "\n",
    "def predict_from_file(path: Path, save: bool = True):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Файл {path} не найден.')\n",
    "    texts = [line.strip() for line in path.read_text(encoding='utf-8').splitlines() if line.strip()]\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        tokens = tokenize_text(text)\n",
    "        tags = predict_tags_for_sentence(tokens) if tokens else []\n",
    "        entities = extract_entities(tokens, tags) if tokens else []\n",
    "        results.append({'text': text, 'tokens': tokens, 'predicted_tags': tags, 'entities': entities})\n",
    "\n",
    "    if save:\n",
    "        out_path = REPORTS_DIR / f'external_predictions_{path.stem}.json'\n",
    "        out_path.write_text(json.dumps(results, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "        print('Сохранено в', out_path)\n",
    "    return results\n",
    "\n",
    "\n",
    "default_text_path = LAB_DIR / 'sample_text.txt'\n",
    "print('Файл по умолчанию:', default_text_path)\n",
    "preview = predict_from_file(default_text_path)\n",
    "for item in preview[:]:\n",
    "    print('---')\n",
    "    print(item['text'])\n",
    "    print('Сущности:', item['entities'] or '—')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
