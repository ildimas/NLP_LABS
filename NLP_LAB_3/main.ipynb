{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058d7d90",
   "metadata": {},
   "source": [
    "# Лабораторная 3: ELMo + BiLSTM NER (standalone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cec912",
   "metadata": {},
   "source": [
    "## 0. Предварительные требования\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776957e2",
   "metadata": {},
   "source": [
    "## 1. Базовые пути и окружение\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55f15b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рабочая папка: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/NLP_LAB_3\n",
      "Данные будут в: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/data\n",
      "Модели будут в: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/models\n",
      "Результаты будем сохранять в: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/NLP_LAB_3/reports\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "LAB_DIR = Path.cwd()\n",
    "if not (LAB_DIR / 'train_elmo_lstm.py').exists():\n",
    "    candidate = LAB_DIR / 'NLP_LAB_3'\n",
    "    if (candidate / 'train_elmo_lstm.py').exists():\n",
    "        LAB_DIR = candidate.resolve()\n",
    "        sys.path.append(str(LAB_DIR))\n",
    "\n",
    "DATA_DIR = (LAB_DIR / '..' / 'data').resolve()\n",
    "MODELS_DIR = (LAB_DIR / '..' / 'models').resolve()\n",
    "REPORTS_DIR = LAB_DIR / 'reports'\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Рабочая папка: {LAB_DIR}\")\n",
    "print(f\"Данные будут в: {DATA_DIR}\")\n",
    "print(f\"Модели будут в: {MODELS_DIR}\")\n",
    "print(f\"Результаты будем сохранять в: {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abab6e",
   "metadata": {},
   "source": [
    "## 2. Установка зависимостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63805f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/il_dimas/Library/Python/3.9/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/il_dimas/Library/Python/3.9/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch==1.12.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: torchvision==0.13.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 2)) (0.13.1)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.5 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.64 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 6)) (4.66.1)\n",
      "Requirement already satisfied: seqeval>=1.2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: overrides==3.1.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: razdel>=0.5.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 9)) (0.5.0)\n",
      "Requirement already satisfied: allennlp==2.10.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 10)) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from torch==1.12.1->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: requests in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from torchvision==0.13.1->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from torchvision==0.13.1->-r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: cached-path<1.2.0,>=1.1.3 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (1.1.6)\n",
      "Requirement already satisfied: fairscale==0.4.6 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: nltk>=3.6.5 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (3.9.2)\n",
      "Requirement already satisfied: spacy<3.4,>=2.1.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (3.3.3)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (2.6.4)\n",
      "Requirement already satisfied: h5py>=3.6.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (3.14.0)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (1.13.1)\n",
      "Requirement already satisfied: pytest>=6.2.5 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (8.4.2)\n",
      "Requirement already satisfied: transformers<4.21,>=4.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (4.20.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (0.2.1)\n",
      "Requirement already satisfied: filelock<3.8,>=3.3 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (3.7.1)\n",
      "Requirement already satisfied: lmdb>=1.2.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (1.7.5)\n",
      "Requirement already satisfied: more-itertools>=8.12.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (10.8.0)\n",
      "Requirement already satisfied: termcolor==1.1.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (1.1.0)\n",
      "Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (0.12.21)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.16 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (0.10.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: base58>=2.1.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: sacremoses in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (0.1.1)\n",
      "Requirement already satisfied: typer>=0.4.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (3.20.3)\n",
      "Requirement already satisfied: traitlets>5.1.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (5.14.3)\n",
      "Requirement already satisfied: jsonnet>=0.10.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from allennlp==2.10.1->-r requirements.txt (line 10)) (0.21.0)\n",
      "Requirement already satisfied: rich<13.0,>=12.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (12.6.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (1.40.67)\n",
      "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (2.19.0)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.67 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (1.40.67)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from botocore<1.41.0,>=1.40.67->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from botocore<1.41.0,>=1.40.67->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (1.26.20)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (2.42.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (1.71.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (1.26.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (4.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.0.16->allennlp==2.10.1->-r requirements.txt (line 10)) (6.0.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.0.16->allennlp==2.10.1->-r requirements.txt (line 10)) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.67->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (1.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 2)) (2025.10.5)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (2.19.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from rsa<5,>=3.1.4->google-auth<3.0dev,>=2.26.1->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp==2.10.1->-r requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (8.0.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (0.7.11)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (2.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (58.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from transformers<4.21,>=4.1->allennlp==2.10.1->-r requirements.txt (line 10)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from transformers<4.21,>=4.1->allennlp==2.10.1->-r requirements.txt (line 10)) (0.11.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from typer>=0.4.1->allennlp==2.10.1->-r requirements.txt (line 10)) (8.1.8)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (3.1.45)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (1.0.13)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (7.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (2.43.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (1.3.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from pandas>=1.5->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from pandas>=1.5->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.1->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.1->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp==2.10.1->-r requirements.txt (line 10)) (5.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from pathy>=0.3.5->spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (0.1.1)\n",
      "Requirement already satisfied: exceptiongroup>=1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from pytest>=6.2.5->allennlp==2.10.1->-r requirements.txt (line 10)) (1.2.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from pytest>=6.2.5->allennlp==2.10.1->-r requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from pytest>=6.2.5->allennlp==2.10.1->-r requirements.txt (line 10)) (1.6.0)\n",
      "Requirement already satisfied: tomli>=1 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from pytest>=6.2.5->allennlp==2.10.1->-r requirements.txt (line 10)) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/il_dimas/Library/Python/3.9/lib/python/site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp==2.10.1->-r requirements.txt (line 10)) (3.0.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/il_dimas/Library/Python/3.9/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/il_dimas/Library/Python/3.9/lib/python/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d0be9",
   "metadata": {},
   "source": [
    "## 3. Загрузка датасета и предобученной ELMo-модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c8e5e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA AND MODEL LOADED!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "DATA_REPO = DATA_DIR / 'Detailed-NER-Dataset-RU'\n",
    "if not DATA_REPO.exists():\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/AlexKly/Detailed-NER-Dataset-RU.git', str(DATA_REPO)], check=True)\n",
    "\n",
    "MODEL_DIR = MODELS_DIR / 'ruwikiruscorpora_tokens_elmo_1024_2019'\n",
    "MODEL_ZIP = MODELS_DIR / 'ruwikiruscorpora_tokens_elmo_1024_2019.zip'\n",
    "MODEL_URL = 'https://vectors.nlpl.eu/repository/20/195.zip'\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(MODEL_URL, MODEL_ZIP)\n",
    "    with zipfile.ZipFile(MODEL_ZIP, 'r') as zf:\n",
    "        zf.extractall(MODELS_DIR / 'ruwikiruscorpora_tokens_elmo_1024_2019')\n",
    "    MODEL_ZIP.unlink(missing_ok=True)\n",
    "\n",
    "print('DATA AND MODEL LOADED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e664176",
   "metadata": {},
   "source": [
    "## 4. Импорт библиотек и конфигурация устройства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10fa9341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13994501",
   "metadata": {},
   "source": [
    "## 5 перевод тегов BIOLU → BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b2acff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U-CITY', 'B-LAST_NAME', 'L-LAST_NAME', 'O'] -> ['B-CITY', 'B-LAST_NAME', 'I-LAST_NAME', 'O']\n"
     ]
    }
   ],
   "source": [
    "def biolu2bio(tags: Sequence[str]) -> List[str]:\n",
    "    converted = []\n",
    "    for tag in tags:\n",
    "        prefix = tag.split('-')[0]\n",
    "        label = tag.split('-')[-1]\n",
    "        if prefix == 'U':\n",
    "            converted.append(f'B-{label}')\n",
    "        elif prefix == 'L':\n",
    "            converted.append(f'I-{label}')\n",
    "        else:\n",
    "            converted.append(tag)\n",
    "    return converted\n",
    "\n",
    "example = ['U-CITY', 'B-LAST_NAME', 'L-LAST_NAME', 'O']\n",
    "print(example, '->', biolu2bio(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbd061",
   "metadata": {},
   "source": [
    "## 6. Загрузка датасета и проверка BIO-тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7928418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO - good\n",
      "предложения 7532\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "pickle_path = DATA_REPO / 'dataset' / 'detailed-ner_dataset-ru.pickle'\n",
    "if not pickle_path.exists():\n",
    "    raise FileNotFoundError('Не найден detailed-ner_dataset-ru.pickle.')\n",
    "\n",
    "df = pd.read_pickle(pickle_path)\n",
    "tokens = df['tokens'].tolist()\n",
    "raw_tags = df['ner_tags'].tolist()\n",
    "\n",
    "bio_tags = [biolu2bio(seq) for seq in raw_tags]\n",
    "\n",
    "invalid = []\n",
    "for seq in bio_tags:\n",
    "    for tag in seq:\n",
    "        if tag != 'O' and not tag.startswith(('B-', 'I-')):\n",
    "            invalid.append(tag)\n",
    "\n",
    "if invalid:\n",
    "    print(Counter(invalid))\n",
    "else:\n",
    "    print('BIO - good')\n",
    "\n",
    "print('предложения', len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0efbd",
   "metadata": {},
   "source": [
    "## 7. Статистика датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "642de885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B-LAST_NAME', 1084), ('B-FIRST_NAME', 918), ('B-COUNTRY', 804), ('B-CITY', 677), ('B-REGION', 381), ('B-MIDDLE_NAME', 311), ('I-HOUSE', 218), ('B-STREET', 135), ('B-HOUSE', 120), ('B-DISTRICT', 110)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tag_counter = Counter(tag for seq in bio_tags for tag in seq if tag != 'O')\n",
    "print(tag_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4be5d4",
   "metadata": {},
   "source": [
    "## 8. Определение словаря тегов и разбиение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d268ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5422, Val: 603, Test: 1507\n",
      "Количество тегов: 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = np.arange(len(tokens))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "def select(items, idxs):\n",
    "    return [items[i] for i in idxs]\n",
    "\n",
    "train_tokens, val_tokens, test_tokens = select(tokens, train_idx), select(tokens, val_idx), select(tokens, test_idx)\n",
    "train_tags, val_tags, test_tags = select(bio_tags, train_idx), select(bio_tags, val_idx), select(bio_tags, test_idx)\n",
    "\n",
    "print(f'Train: {len(train_tokens)}, Val: {len(val_tokens)}, Test: {len(test_tokens)}')\n",
    "\n",
    "unique_tags = sorted({tag for seq in bio_tags for tag in seq})\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "print('Количество тегов:', len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce0597",
   "metadata": {},
   "source": [
    "## 9. Обёртки для ELMo и Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1ebf442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoSentenceEmbedder:\n",
    "    def __init__(self, options_path: Path, weight_path: Path, device: torch.device):\n",
    "        self.device = device\n",
    "        self.elmo = Elmo(\n",
    "            options_file=str(options_path),\n",
    "            weight_file=str(weight_path),\n",
    "            num_output_representations=1,\n",
    "            dropout=0.0,\n",
    "        ).to(self.device)\n",
    "        self.elmo.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(self, tokens: Sequence[str]) -> torch.Tensor:\n",
    "        character_ids = batch_to_ids([list(tokens)]).to(self.device)\n",
    "        outputs = self.elmo(character_ids)\n",
    "        embeddings = outputs['elmo_representations'][0][0]\n",
    "        return embeddings.cpu()\n",
    "\n",
    "\n",
    "class ElmoSequenceDataset(Dataset):\n",
    "    def __init__(self, tokens: Sequence[Sequence[str]], tags: Sequence[Sequence[str]], embedder: ElmoSentenceEmbedder, tag2idx: Dict[str, int]):\n",
    "        self.tokens = tokens\n",
    "        self.tags = tags\n",
    "        self.embedder = embedder\n",
    "        self.tag2idx = tag2idx\n",
    "        self._cache: Dict[int, torch.Tensor] = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def _embed(self, idx: int) -> torch.Tensor:\n",
    "        if idx not in self._cache:\n",
    "            embedding = self.embedder.embed(self.tokens[idx])\n",
    "            self._cache[idx] = embedding.clone().detach()\n",
    "        return self._cache[idx]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        embeddings = self._embed(idx)\n",
    "        tag_ids = torch.tensor([self.tag2idx[tag] for tag in self.tags[idx]], dtype=torch.long)\n",
    "        return embeddings, tag_ids, embeddings.size(0)\n",
    "\n",
    "\n",
    "def collate_batch(batch: Iterable[Tuple[torch.Tensor, torch.Tensor, int]]):\n",
    "    embeddings, tags, lengths = zip(*batch)\n",
    "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
    "    padded_embeddings = pad_sequence(embeddings, batch_first=True)\n",
    "    padded_tags = pad_sequence(tags, batch_first=True, padding_value=-100)\n",
    "    return padded_embeddings, padded_tags, lengths_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223f56a6",
   "metadata": {},
   "source": [
    "## 10. BiLSTM модель и утилиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3aa8b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLstmTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_size: int, num_labels: int, num_layers: int = 1, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size * 2, num_labels)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        outputs, _ = self.lstm(packed)\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "        outputs = self.dropout(outputs)\n",
    "        return self.classifier(outputs)\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    loss: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    pbar = tqdm(loader, desc='Train', leave=False)\n",
    "    for embeddings, tags, lengths in pbar:\n",
    "        embeddings = embeddings.to(DEVICE)\n",
    "        tags = tags.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(embeddings, lengths)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), tags.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        valid_tokens = (tags != -100).sum().item()\n",
    "        total_loss += loss.item() * valid_tokens\n",
    "        total_tokens += valid_tokens\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.04f}'})\n",
    "    return total_loss / max(total_tokens, 1)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, idx2tag, stage='Eval'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    gold_sequences, pred_sequences = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for embeddings, tags, lengths in tqdm(loader, desc=stage, leave=False):\n",
    "            embeddings = embeddings.to(DEVICE)\n",
    "            tags = tags.to(DEVICE)\n",
    "            logits = model(embeddings, lengths)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tags.view(-1))\n",
    "\n",
    "            mask = tags != -100\n",
    "            token_count = mask.sum().item()\n",
    "            total_loss += loss.item() * token_count\n",
    "            total_tokens += token_count\n",
    "\n",
    "            preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "            gold = tags.cpu().numpy()\n",
    "            lengths_np = lengths.numpy()\n",
    "            for pred_seq, gold_seq, length in zip(preds, gold, lengths_np):\n",
    "                pred_labels = [idx2tag[idx] for idx in pred_seq[:length]]\n",
    "                gold_labels = [idx2tag[idx] for idx in gold_seq[:length]]\n",
    "                pred_sequences.append(pred_labels)\n",
    "                gold_sequences.append(gold_labels)\n",
    "\n",
    "    loss_value = total_loss / max(total_tokens, 1)\n",
    "    precision = precision_score(gold_sequences, pred_sequences, zero_division=0)\n",
    "    recall = recall_score(gold_sequences, pred_sequences, zero_division=0)\n",
    "    f1 = f1_score(gold_sequences, pred_sequences, zero_division=0)\n",
    "    return Metrics(loss_value, precision, recall, f1), gold_sequences, pred_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c7645",
   "metadata": {},
   "source": [
    "## 11. Гиперпараметры и DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d29c906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL AND DATA READY!\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'epochs': 5,\n",
    "    'batch_size': 12,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 1e-3,\n",
    "    'seed': 42,\n",
    "}\n",
    "set_seed(config['seed'])\n",
    "options_path = MODEL_DIR / 'options.json'\n",
    "weights_path = MODEL_DIR / 'model.hdf5'\n",
    "if not (options_path.exists() and weights_path.exists()):\n",
    "    raise FileNotFoundError('Не найдены options.json/model.hdf5 в директории модели.')\n",
    "\n",
    "elmo_embedder = ElmoSentenceEmbedder(options_path, weights_path, DEVICE)\n",
    "\n",
    "train_dataset = ElmoSequenceDataset(train_tokens, train_tags, elmo_embedder, tag2idx)\n",
    "val_dataset = ElmoSequenceDataset(val_tokens, val_tags, elmo_embedder, tag2idx)\n",
    "test_dataset = ElmoSequenceDataset(test_tokens, test_tags, elmo_embedder, tag2idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "model = BiLstmTagger(\n",
    "    embedding_dim=1024,\n",
    "    hidden_size=config['hidden_size'],\n",
    "    num_labels=len(tag2idx),\n",
    "    num_layers=config['num_layers'],\n",
    "    dropout=config['dropout'],\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "print('MODEL AND DATA READY!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03784138",
   "metadata": {},
   "source": [
    "## 12. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c17433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2156\n",
      "Validation -> loss: 0.1636, precision: 0.7731, recall: 0.6411, f1: 0.7009\n",
      "=== Epoch 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0852\n",
      "Validation -> loss: 0.1182, precision: 0.7705, recall: 0.7896, f1: 0.7800\n",
      "=== Epoch 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0584\n",
      "Validation -> loss: 0.0974, precision: 0.8317, recall: 0.8193, f1: 0.8254\n",
      "=== Epoch 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0430\n",
      "Validation -> loss: 0.0923, precision: 0.8038, recall: 0.8317, f1: 0.8175\n",
      "=== Epoch 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0280\n",
      "Validation -> loss: 0.0844, precision: 0.8601, recall: 0.8366, f1: 0.8482\n",
      "(5).\n",
      "Обучение заняло 5.48 мин.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "best_state = None\n",
    "best_val_f1 = -1.0\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    print(f'=== Epoch {epoch}/{config[\"epochs\"]} ===')\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_metrics, _, _ = evaluate(model, val_loader, criterion, idx2tag, stage='Validation')\n",
    "\n",
    "    epoch_summary = {\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_metrics.loss,\n",
    "        'val_precision': val_metrics.precision,\n",
    "        'val_recall': val_metrics.recall,\n",
    "        'val_f1': val_metrics.f1,\n",
    "    }\n",
    "    history.append(epoch_summary)\n",
    "    print(f\"Train loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation -> loss: {val_metrics.loss:.4f}, precision: {val_metrics.precision:.4f}, recall: {val_metrics.recall:.4f}, f1: {val_metrics.f1:.4f}\")\n",
    "\n",
    "    if val_metrics.f1 > best_val_f1:\n",
    "        best_val_f1 = val_metrics.f1\n",
    "        best_state = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_metrics': val_metrics.to_dict(),\n",
    "        }\n",
    "\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state['model'])\n",
    "    print(f\"({best_state['epoch']}).\")\n",
    "\n",
    "print(f\"Обучение заняло {(time.time() - start_time)/60:.2f} мин.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9695bae4",
   "metadata": {},
   "source": [
    "## 13. Тестирование и отчёт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b98e2eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test -> loss: 0.0797, precision: 0.8571, recall: 0.8230, f1: 0.8397\n",
      "Seqeval report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CITY       0.95      0.90      0.92       148\n",
      "     COUNTRY       0.88      0.86      0.87       132\n",
      "    DISTRICT       0.92      0.71      0.80        17\n",
      "  FIRST_NAME       0.81      0.82      0.82       194\n",
      "       HOUSE       0.63      0.71      0.67        24\n",
      "   LAST_NAME       0.83      0.80      0.81       235\n",
      " MIDDLE_NAME       0.86      0.80      0.83        60\n",
      "      REGION       0.93      0.78      0.85        69\n",
      "      STREET       0.86      0.76      0.81        25\n",
      "\n",
      "   micro avg       0.86      0.82      0.84       904\n",
      "   macro avg       0.85      0.79      0.82       904\n",
      "weighted avg       0.86      0.82      0.84       904\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test_metrics, gold_sequences, pred_sequences = evaluate(model, test_loader, criterion, idx2tag, stage='Test')\n",
    "print(\n",
    "    f\"Test -> loss: {test_metrics.loss:.4f}, precision: {test_metrics.precision:.4f}, \"\n",
    "    f\"recall: {test_metrics.recall:.4f}, f1: {test_metrics.f1:.4f}\"\n",
    ")\n",
    "\n",
    "report_text = classification_report(gold_sequences, pred_sequences, zero_division=0)\n",
    "print('Seqeval report:')\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec6f0a",
   "metadata": {},
   "source": [
    "## 14. Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a25077d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2385"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_payload = {\n",
    "    'config': config,\n",
    "    'history': history,\n",
    "    'best_val': best_state['val_metrics'] if best_state else None,\n",
    "    'test': test_metrics.to_dict(),\n",
    "    'classification_report': report_text,\n",
    "}\n",
    "\n",
    "metrics_path = REPORTS_DIR / 'metrics.json'\n",
    "metrics_path.write_text(json.dumps(metrics_payload, indent=2, ensure_ascii=False), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383bbed",
   "metadata": {},
   "source": [
    "## 15. Инференс на новых текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0af5205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл по умолчанию: /Users/il_dimas/Documents/Programming_projects/NLP_LABS/NLP_LAB_3/sample_text.txt\n",
      "Сохранено в /Users/il_dimas/Documents/Programming_projects/NLP_LABS/NLP_LAB_3/reports/external_predictions_sample_text.json\n",
      "---\n",
      "Мэр Москвы Сергей Собянин объявил о запуске новой линии МЦД между Одинцово и Лобней.\n",
      "Сущности: [('Москвы', 'CITY'), ('Сергей', 'FIRST_NAME'), ('Собянин', 'LAST_NAME'), ('Одинцово', 'DISTRICT'), ('Лобней', 'DISTRICT')]\n",
      "---\n",
      "РБК со ссылкой на источники сообщил, что Минфин готовит пакет налоговых льгот для IT-компаний.\n",
      "Сущности: —\n",
      "---\n",
      "Газпром договорился с властями Санкт-Петербурга о строительстве второй ветки «Лахта центру».\n",
      "Сущности: [('Санкт-Петербурга', 'CITY')]\n",
      "---\n",
      "Губернатор Краснодарского края Вениамин Кондратьев открыл обновленный аэропорт Сочи.\n",
      "Сущности: [('Краснодарского', 'REGION'), ('Вениамин', 'FIRST_NAME'), ('Кондратьев', 'LAST_NAME'), ('Сочи', 'CITY')]\n",
      "---\n",
      "Госдума рассмотрит проект о введении электронных гарантий для экспортеров сельхозпродукции.\n",
      "Сущности: —\n",
      "---\n",
      "Компания Яндекс представила новый сервис доставки продуктов в Казани и Нижнем Новгороде.\n",
      "Сущности: [('Казани', 'CITY'), ('Нижнем Новгороде', 'CITY')]\n",
      "---\n",
      "Суд в Татарстане арестовал бизнесмена Рустама Салахутдинова по делу о хищениях.\n",
      "Сущности: [('Татарстане', 'REGION'), ('Рустама', 'FIRST_NAME'), ('Салахутдинова', 'LAST_NAME')]\n",
      "---\n",
      "Росатом начнет строительство ветропарка в Мурманской области в 2025 году.\n",
      "Сущности: [('Мурманской', 'REGION')]\n",
      "---\n",
      "РЖД планируют модернизировать вокзал Курского направления в Москве.\n",
      "Сущности: [('Курского', 'DISTRICT'), ('Москве', 'CITY')]\n",
      "---\n",
      "Банк ВТБ открыл консультационный офис для малого бизнеса в Новосибирске.\n",
      "Сущности: [('Новосибирске', 'CITY')]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    return [token.text for token in razdel_tokenize(text)]\n",
    "\n",
    "\n",
    "def extract_entities(tokens: Sequence[str], tags: Sequence[str]):\n",
    "    entities = []\n",
    "    buffer = []\n",
    "    current_label = None\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag == 'O':\n",
    "            if buffer:\n",
    "                entities.append((' '.join(buffer), current_label))\n",
    "                buffer = []\n",
    "                current_label = None\n",
    "            continue\n",
    "        prefix, label = tag.split('-', 1)\n",
    "        if prefix == 'B':\n",
    "            if buffer:\n",
    "                entities.append((' '.join(buffer), current_label))\n",
    "            buffer = [token]\n",
    "            current_label = label\n",
    "        elif prefix == 'I' and current_label == label:\n",
    "            buffer.append(token)\n",
    "        else:\n",
    "            if buffer:\n",
    "                entities.append((' '.join(buffer), current_label))\n",
    "            buffer = [token]\n",
    "            current_label = label\n",
    "    if buffer:\n",
    "        entities.append((' '.join(buffer), current_label))\n",
    "    return entities\n",
    "\n",
    "\n",
    "def predict_tags_for_sentence(tokens: Sequence[str]):\n",
    "    embeddings = elmo_embedder.embed(tokens)\n",
    "    tensor = embeddings.unsqueeze(0).to(DEVICE)\n",
    "    lengths = torch.tensor([len(tokens)], dtype=torch.long)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor, lengths)\n",
    "        pred_ids = logits.argmax(dim=-1)[0][: len(tokens)].cpu().tolist()\n",
    "    return [idx2tag[idx] for idx in pred_ids]\n",
    "\n",
    "\n",
    "def predict_from_file(path: Path, save: bool = True):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Файл {path} не найден.')\n",
    "    texts = [line.strip() for line in path.read_text(encoding='utf-8').splitlines() if line.strip()]\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        tokens = tokenize_text(text)\n",
    "        tags = predict_tags_for_sentence(tokens) if tokens else []\n",
    "        entities = extract_entities(tokens, tags) if tokens else []\n",
    "        results.append({'text': text, 'tokens': tokens, 'predicted_tags': tags, 'entities': entities})\n",
    "\n",
    "    if save:\n",
    "        out_path = REPORTS_DIR / f'external_predictions_{path.stem}.json'\n",
    "        out_path.write_text(json.dumps(results, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "        print('Сохранено в', out_path)\n",
    "    return results\n",
    "\n",
    "\n",
    "default_text_path = LAB_DIR / 'sample_text.txt'\n",
    "print('Файл по умолчанию:', default_text_path)\n",
    "preview = predict_from_file(default_text_path)\n",
    "for item in preview[:]:\n",
    "    print('---')\n",
    "    print(item['text'])\n",
    "    print('Сущности:', item['entities'] or '—')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
